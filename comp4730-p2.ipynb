{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-12-11T18:58:48.738066Z","iopub.status.busy":"2022-12-11T18:58:48.737695Z","iopub.status.idle":"2022-12-11T18:58:56.153581Z","shell.execute_reply":"2022-12-11T18:58:56.152597Z","shell.execute_reply.started":"2022-12-11T18:58:48.737987Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from sklearn.model_selection import train_test_split\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:58:56.156349Z","iopub.status.busy":"2022-12-11T18:58:56.155662Z","iopub.status.idle":"2022-12-11T18:58:57.452148Z","shell.execute_reply":"2022-12-11T18:58:57.451222Z","shell.execute_reply.started":"2022-12-11T18:58:56.156310Z"},"trusted":true},"outputs":[],"source":["INPUT_DIR = os.path.join('..', 'input')\n","\n","DATASET_DIR = os.path.join(INPUT_DIR, 'landmark-recognition-2021')\n","TEST_IMAGE_DIR = os.path.join(DATASET_DIR, 'test')\n","TRAIN_IMAGE_DIR = os.path.join(DATASET_DIR, 'train')\n","TRAIN_LABELMAP_PATH = os.path.join(DATASET_DIR, 'train.csv')\n","\n","df_train = pd.read_csv(TRAIN_LABELMAP_PATH)\n","df_test = pd.read_csv(f\"{DATASET_DIR}/sample_submission.csv\")"]},{"cell_type":"markdown","metadata":{},"source":["# **Hyperparameters**"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:40.078732Z","iopub.status.busy":"2022-12-11T18:59:40.077914Z","iopub.status.idle":"2022-12-11T18:59:40.104903Z","shell.execute_reply":"2022-12-11T18:59:40.103982Z","shell.execute_reply.started":"2022-12-11T18:59:40.078688Z"},"trusted":true},"outputs":[],"source":["input_shape = (256, 256, 3)\n","patch_size = (4, 4)  # 2-by-2 sized patches\n","dropout_rate = 0.03  # Dropout rate\n","num_heads = 8  # Attention heads\n","embed_dim = 64  # Embedding dimension\n","num_mlp = 256  # MLP layer size\n","qkv_bias = True  # Convert embedded patches to query, key, and values with a learnable additive value\n","window_size = 8  # Size of attention window\n","shift_size = 1  # Size of shifting window\n","image_dimension = 256  # Initial image size\n","\n","num_patch_x = input_shape[0] // patch_size[0]\n","num_patch_y = input_shape[1] // patch_size[1]\n","\n","learning_rate = 1e-3\n","batch_size = 32\n","num_epochs = 40\n","weight_decay = 0.0001\n","seed = 24\n","top_k = 5 # Number of retrieved images used to make prediction for a test image.\n","\n","n_classes = df_train['landmark_id'].nunique()\n","threshold = 0.5\n","\n","AUTO = tf.data.experimental.AUTOTUNE"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:58:57.493882Z","iopub.status.busy":"2022-12-11T18:58:57.493172Z","iopub.status.idle":"2022-12-11T18:59:00.635191Z","shell.execute_reply":"2022-12-11T18:59:00.634217Z","shell.execute_reply.started":"2022-12-11T18:58:57.493855Z"},"trusted":true},"outputs":[],"source":["df_train['path']=[TRAIN_IMAGE_DIR+'/'+id[0]+'/'+id[1]+'/'+id[2]+'/'+id+'.jpg' for id in df_train['id']]\n","\n","\n","df_train['id_counts'] = df_train.landmark_id.value_counts().loc[df_train.landmark_id.values].values\n","id_map = df_train.sort_values(by='id_counts').landmark_id.drop_duplicates().reset_index(drop=True)\n","id_dict = {id_map.loc[x]:n_classes-x for x in range(n_classes)}\n","df_train['encode_id'] = df_train.landmark_id.apply(lambda x: id_dict[x])\n","\n","df_test['path']=[TEST_IMAGE_DIR+'/'+id[0]+'/'+id[1]+'/'+id[2]+'/'+id+'.jpg' for id in df_test['id']]"]},{"cell_type":"markdown","metadata":{},"source":["# **Helper Functions**"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:00.637487Z","iopub.status.busy":"2022-12-11T18:59:00.636610Z","iopub.status.idle":"2022-12-11T18:59:00.655007Z","shell.execute_reply":"2022-12-11T18:59:00.654103Z","shell.execute_reply.started":"2022-12-11T18:59:00.637450Z"},"trusted":true},"outputs":[],"source":["def read_image_and_label(image_path, label=None,resize=[input_shape[0], input_shape[1]]):\n","    image = tf.io.read_file(image_path)\n","    image = tf.image.decode_jpeg(image, channels=input_shape[2])\n","    image = tf.image.resize(image,[input_shape[0], input_shape[1]])\n","    image = tf.cast(image, dtype=tf.float32)/255.\n","    if not label is None:\n","        return image, label\n","    return image\n","\n","def get_training_dataset(df):\n","    training_dataset = tf.data.Dataset.from_tensor_slices((df[\"path\"].values, df[\"encode_id\"].values))\n","    training_dataset = training_dataset.map(read_image_and_label,num_parallel_calls=AUTO)\n","    training_dataset = training_dataset.shuffle(1000, reshuffle_each_iteration=True)\n","    training_dataset = training_dataset.batch(batch_size)\n","    training_dataset = training_dataset.prefetch(AUTO)\n","\n","    return training_dataset\n","\n","def get_validation_dataset(df,batch_size=batch_size):\n","  validation_dataset = tf.data.Dataset.from_tensor_slices((df[\"path\"].values, df[\"encode_id\"].values))\n","  validation_dataset = validation_dataset.map(read_image_and_label)\n","  validation_dataset = validation_dataset.batch(batch_size)\n","\n","  return validation_dataset\n","\n","def get_test_dataset(images,batch_size=batch_size): \n","  test_dataset = tf.data.Dataset.from_tensor_slices((images))\n","  test_dataset = test_dataset.map(read_image_and_label)\n","  test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n","\n","  return test_dataset\n","\n","def window_partition(x, window_size):\n","    _, height, width, channels = x.shape\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = tf.reshape(\n","        x, shape=(-1, patch_num_y, window_size, patch_num_x, window_size, channels)\n","    )\n","    x = tf.transpose(x, (0, 1, 3, 2, 4, 5))\n","    windows = tf.reshape(x, shape=(-1, window_size, window_size, channels))\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, height, width, channels):\n","    patch_num_y = height // window_size\n","    patch_num_x = width // window_size\n","    x = tf.reshape(\n","        windows,\n","        shape=(-1, patch_num_y, patch_num_x, window_size, window_size, channels),\n","    )\n","    x = tf.transpose(x, perm=(0, 1, 3, 2, 4, 5))\n","    x = tf.reshape(x, shape=(-1, height, width, channels))\n","    return x\n","\n","\n","class DropPath(layers.Layer):\n","    def __init__(self, drop_prob=None, **kwargs):\n","        super(DropPath, self).__init__(**kwargs)\n","        self.drop_prob = drop_prob\n","\n","    def call(self, x):\n","        input_shape = tf.shape(x)\n","        batch_size = input_shape[0]\n","        rank = x.shape.rank\n","        shape = (batch_size,) + (1,) * (rank - 1)\n","        random_tensor = (1 - self.drop_prob) + tf.random.uniform(shape, dtype=x.dtype)\n","        path_mask = tf.floor(random_tensor)\n","        output = tf.math.divide(x, 1 - self.drop_prob) * path_mask\n","        return output\n"]},{"cell_type":"markdown","metadata":{},"source":["# **Models**"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:00.656996Z","iopub.status.busy":"2022-12-11T18:59:00.656635Z","iopub.status.idle":"2022-12-11T18:59:00.678422Z","shell.execute_reply":"2022-12-11T18:59:00.677517Z","shell.execute_reply.started":"2022-12-11T18:59:00.656959Z"},"trusted":true},"outputs":[],"source":["class WindowAttention(layers.Layer):\n","    def __init__(\n","        self, dim, window_size, num_heads, qkv_bias=True, dropout_rate=0.0, **kwargs\n","    ):\n","        super(WindowAttention, self).__init__(**kwargs)\n","        self.dim = dim\n","        self.window_size = window_size\n","        self.num_heads = num_heads\n","        self.scale = (dim // num_heads) ** -0.5\n","        self.qkv = layers.Dense(dim * 3, use_bias=qkv_bias)\n","        self.dropout = layers.Dropout(dropout_rate)\n","        self.proj = layers.Dense(dim)\n","\n","    def build(self, input_shape):\n","        num_window_elements = (2 * self.window_size[0] - 1) * (\n","            2 * self.window_size[1] - 1\n","        )\n","        self.relative_position_bias_table = self.add_weight(\n","            shape=(num_window_elements, self.num_heads),\n","            initializer=tf.initializers.Zeros(),\n","            trainable=True,\n","        )\n","        coords_h = np.arange(self.window_size[0])\n","        coords_w = np.arange(self.window_size[1])\n","        coords_matrix = np.meshgrid(coords_h, coords_w, indexing=\"ij\")\n","        coords = np.stack(coords_matrix)\n","        coords_flatten = coords.reshape(2, -1)\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n","        relative_coords = relative_coords.transpose([1, 2, 0])\n","        relative_coords[:, :, 0] += self.window_size[0] - 1\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)\n","\n","        self.relative_position_index = tf.Variable(\n","            initial_value=tf.convert_to_tensor(relative_position_index), trainable=False\n","        )\n","\n","    def call(self, x, mask=None):\n","        _, size, channels = x.shape\n","        head_dim = channels // self.num_heads\n","        x_qkv = self.qkv(x)\n","        x_qkv = tf.reshape(x_qkv, shape=(-1, size, 3, self.num_heads, head_dim))\n","        x_qkv = tf.transpose(x_qkv, perm=(2, 0, 3, 1, 4))\n","        q, k, v = x_qkv[0], x_qkv[1], x_qkv[2]\n","        q = q * self.scale\n","        k = tf.transpose(k, perm=(0, 1, 3, 2))\n","        attn = q @ k\n","\n","        num_window_elements = self.window_size[0] * self.window_size[1]\n","        relative_position_index_flat = tf.reshape(\n","            self.relative_position_index, shape=(-1,)\n","        )\n","        relative_position_bias = tf.gather(\n","            self.relative_position_bias_table, relative_position_index_flat\n","        )\n","        relative_position_bias = tf.reshape(\n","            relative_position_bias, shape=(num_window_elements, num_window_elements, -1)\n","        )\n","        relative_position_bias = tf.transpose(relative_position_bias, perm=(2, 0, 1))\n","        attn = attn + tf.expand_dims(relative_position_bias, axis=0)\n","\n","        if mask is not None:\n","            nW = mask.get_shape()[0]\n","            mask_float = tf.cast(\n","                tf.expand_dims(tf.expand_dims(mask, axis=1), axis=0), tf.float32\n","            )\n","            attn = (\n","                tf.reshape(attn, shape=(-1, nW, self.num_heads, size, size))\n","                + mask_float\n","            )\n","            attn = tf.reshape(attn, shape=(-1, self.num_heads, size, size))\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        else:\n","            attn = keras.activations.softmax(attn, axis=-1)\n","        attn = self.dropout(attn)\n","\n","        x_qkv = attn @ v\n","        x_qkv = tf.transpose(x_qkv, perm=(0, 2, 1, 3))\n","        x_qkv = tf.reshape(x_qkv, shape=(-1, size, channels))\n","        x_qkv = self.proj(x_qkv)\n","        x_qkv = self.dropout(x_qkv)\n","        return x_qkv"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:00.681200Z","iopub.status.busy":"2022-12-11T18:59:00.680948Z","iopub.status.idle":"2022-12-11T18:59:00.703828Z","shell.execute_reply":"2022-12-11T18:59:00.702935Z","shell.execute_reply.started":"2022-12-11T18:59:00.681176Z"},"trusted":true},"outputs":[],"source":["class SwinTransformer(layers.Layer):\n","    def __init__(\n","        self,\n","        dim,\n","        num_patch,\n","        num_heads,\n","        window_size=8,\n","        shift_size=0,\n","        num_mlp=1024,\n","        qkv_bias=True,\n","        dropout_rate=0.0,\n","        **kwargs,\n","    ):\n","        super(SwinTransformer, self).__init__(**kwargs)\n","\n","        self.dim = dim  # number of input dimensions\n","        self.num_patch = num_patch  # number of embedded patches\n","        self.num_heads = num_heads  # number of attention heads\n","        self.window_size = window_size  # size of window\n","        self.shift_size = shift_size  # size of window shift\n","        self.num_mlp = num_mlp  # number of MLP nodes\n","\n","        self.norm1 = layers.LayerNormalization(epsilon=1e-5)\n","        self.attn = WindowAttention(\n","            dim,\n","            window_size=(self.window_size, self.window_size),\n","            num_heads=num_heads,\n","            qkv_bias=qkv_bias,\n","            dropout_rate=dropout_rate,\n","        )\n","        self.drop_path = DropPath(dropout_rate)\n","        self.norm2 = layers.LayerNormalization(epsilon=1e-5)\n","\n","        self.mlp = keras.Sequential(\n","            [\n","                layers.Dense(num_mlp),\n","                layers.Activation(keras.activations.gelu),\n","                layers.Dropout(dropout_rate),\n","                layers.Dense(dim),\n","                layers.Dropout(dropout_rate),\n","            ]\n","        )\n","\n","        if min(self.num_patch) < self.window_size:\n","            self.shift_size = 0\n","            self.window_size = min(self.num_patch)\n","\n","    def build(self, input_shape):\n","        if self.shift_size == 0:\n","            self.attn_mask = None\n","        else:\n","            height, width = self.num_patch\n","            h_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            w_slices = (\n","                slice(0, -self.window_size),\n","                slice(-self.window_size, -self.shift_size),\n","                slice(-self.shift_size, None),\n","            )\n","            mask_array = np.zeros((1, height, width, 1))\n","            count = 0\n","            for h in h_slices:\n","                for w in w_slices:\n","                    mask_array[:, h, w, :] = count\n","                    count += 1\n","            mask_array = tf.convert_to_tensor(mask_array)\n","\n","            # mask array to windows\n","            mask_windows = window_partition(mask_array, self.window_size)\n","            mask_windows = tf.reshape(\n","                mask_windows, shape=[-1, self.window_size * self.window_size]\n","            )\n","            attn_mask = tf.expand_dims(mask_windows, axis=1) - tf.expand_dims(\n","                mask_windows, axis=2\n","            )\n","            attn_mask = tf.where(attn_mask != 0, -100.0, attn_mask)\n","            attn_mask = tf.where(attn_mask == 0, 0.0, attn_mask)\n","            self.attn_mask = tf.Variable(initial_value=attn_mask, trainable=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, num_patches_before, channels = x.shape\n","        x_skip = x\n","        x = self.norm1(x)\n","        x = tf.reshape(x, shape=(-1, height, width, channels))\n","        if self.shift_size > 0:\n","            shifted_x = tf.roll(\n","                x, shift=[-self.shift_size, -self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            shifted_x = x\n","\n","        x_windows = window_partition(shifted_x, self.window_size)\n","        x_windows = tf.reshape(\n","            x_windows, shape=(-1, self.window_size * self.window_size, channels)\n","        )\n","        attn_windows = self.attn(x_windows, mask=self.attn_mask)\n","\n","        attn_windows = tf.reshape(\n","            attn_windows, shape=(-1, self.window_size, self.window_size, channels)\n","        )\n","        shifted_x = window_reverse(\n","            attn_windows, self.window_size, height, width, channels\n","        )\n","        if self.shift_size > 0:\n","            x = tf.roll(\n","                shifted_x, shift=[self.shift_size, self.shift_size], axis=[1, 2]\n","            )\n","        else:\n","            x = shifted_x\n","\n","        x = tf.reshape(x, shape=(-1, height * width, channels))\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        x_skip = x\n","        x = self.norm2(x)\n","        x = self.mlp(x)\n","        x = self.drop_path(x)\n","        x = x_skip + x\n","        return x"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:00.706000Z","iopub.status.busy":"2022-12-11T18:59:00.705426Z","iopub.status.idle":"2022-12-11T18:59:00.721221Z","shell.execute_reply":"2022-12-11T18:59:00.720320Z","shell.execute_reply.started":"2022-12-11T18:59:00.705958Z"},"trusted":true},"outputs":[],"source":["class PatchExtract(layers.Layer):\n","    def __init__(self, patch_size, **kwargs):\n","        super(PatchExtract, self).__init__(**kwargs)\n","        self.patch_size_x = patch_size[0]\n","        self.patch_size_y = patch_size[0]\n","\n","    def call(self, images):\n","        batch_size = tf.shape(images)[0]\n","        patches = tf.image.extract_patches(\n","            images=images,\n","            sizes=(1, self.patch_size_x, self.patch_size_y, 1),\n","            strides=(1, self.patch_size_x, self.patch_size_y, 1),\n","            rates=(1, 1, 1, 1),\n","            padding=\"VALID\",\n","        )\n","        patch_dim = patches.shape[-1]\n","        patch_num = patches.shape[1]\n","        return tf.reshape(patches, (batch_size, patch_num * patch_num, patch_dim))\n","\n","\n","class PatchEmbedding(layers.Layer):\n","    def __init__(self, num_patch, embed_dim, **kwargs):\n","        super(PatchEmbedding, self).__init__(**kwargs)\n","        self.num_patch = num_patch\n","        self.proj = layers.Dense(embed_dim)\n","        self.pos_embed = layers.Embedding(input_dim=num_patch, output_dim=embed_dim)\n","\n","    def call(self, patch):\n","        pos = tf.range(start=0, limit=self.num_patch, delta=1)\n","        return self.proj(patch) + self.pos_embed(pos)\n","\n","\n","class PatchMerging(tf.keras.layers.Layer):\n","    def __init__(self, num_patch, embed_dim):\n","        super(PatchMerging, self).__init__()\n","        self.num_patch = num_patch\n","        self.embed_dim = embed_dim\n","        self.linear_trans = layers.Dense(2 * embed_dim, use_bias=False)\n","\n","    def call(self, x):\n","        height, width = self.num_patch\n","        _, _, C = x.get_shape().as_list()\n","        x = tf.reshape(x, shape=(-1, height, width, C))\n","        x0 = x[:, 0::2, 0::2, :]\n","        x1 = x[:, 1::2, 0::2, :]\n","        x2 = x[:, 0::2, 1::2, :]\n","        x3 = x[:, 1::2, 1::2, :]\n","        x = tf.concat((x0, x1, x2, x3), axis=-1)\n","        x = tf.reshape(x, shape=(-1, (height // 2) * (width // 2), 4 * C))\n","        return self.linear_trans(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:00.723241Z","iopub.status.busy":"2022-12-11T18:59:00.722854Z","iopub.status.idle":"2022-12-11T18:59:05.856210Z","shell.execute_reply":"2022-12-11T18:59:05.855208Z","shell.execute_reply.started":"2022-12-11T18:59:00.723190Z"},"trusted":true},"outputs":[],"source":["input = layers.Input(input_shape)\n","x = layers.RandomCrop(image_dimension, image_dimension)(input)\n","x = layers.RandomFlip(\"horizontal\")(x)\n","x = PatchExtract(patch_size)(x)\n","x = PatchEmbedding(num_patch_x * num_patch_y, embed_dim)(x)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=0,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = SwinTransformer(\n","    dim=embed_dim,\n","    num_patch=(num_patch_x, num_patch_y),\n","    num_heads=num_heads,\n","    window_size=window_size,\n","    shift_size=shift_size,\n","    num_mlp=num_mlp,\n","    qkv_bias=qkv_bias,\n","    dropout_rate=dropout_rate,\n",")(x)\n","x = PatchMerging((num_patch_x, num_patch_y), embed_dim=embed_dim)(x)\n","x = layers.GlobalAveragePooling1D()(x)\n","output = layers.Dense(n_classes, activation=\"softmax\")(x)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:05.865709Z","iopub.status.busy":"2022-12-11T18:59:05.863210Z","iopub.status.idle":"2022-12-11T18:59:06.066248Z","shell.execute_reply":"2022-12-11T18:59:06.065292Z","shell.execute_reply.started":"2022-12-11T18:59:05.865664Z"},"trusted":true},"outputs":[],"source":["tr_dataset, val_dataset = train_test_split(df_train[:50000], random_state=seed, test_size=.25)\n","tr_dataset = get_training_dataset(tr_dataset)\n","val_dataset = get_validation_dataset(val_dataset)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:06.068037Z","iopub.status.busy":"2022-12-11T18:59:06.067687Z","iopub.status.idle":"2022-12-11T18:59:06.104412Z","shell.execute_reply":"2022-12-11T18:59:06.103396Z","shell.execute_reply.started":"2022-12-11T18:59:06.068000Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 256, 256, 3)]     0         \n","_________________________________________________________________\n","random_crop (RandomCrop)     (None, 256, 256, 3)       0         \n","_________________________________________________________________\n","random_flip (RandomFlip)     (None, 256, 256, 3)       0         \n","_________________________________________________________________\n","patch_extract (PatchExtract) (None, 4096, 48)          0         \n","_________________________________________________________________\n","patch_embedding (PatchEmbedd (None, 4096, 64)          265280    \n","_________________________________________________________________\n","swin_transformer (SwinTransf (None, 4096, 64)          55880     \n","_________________________________________________________________\n","swin_transformer_1 (SwinTran (None, 4096, 64)          318024    \n","_________________________________________________________________\n","patch_merging (PatchMerging) (None, 1024, 128)         32768     \n","_________________________________________________________________\n","global_average_pooling1d (Gl (None, 128)               0         \n","_________________________________________________________________\n","dense_10 (Dense)             (None, 81313)             10489377  \n","=================================================================\n","Total params: 11,161,329\n","Trainable params: 10,890,993\n","Non-trainable params: 270,336\n","_________________________________________________________________\n"]}],"source":["model = keras.Model(input, output)\n","optimizer = tfa.optimizers.AdamW(\n","    learning_rate=learning_rate, weight_decay=weight_decay\n",")\n","model.compile(\n","    optimizer=optimizer,\n","    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n","    metrics=[\n","        keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n","        keras.metrics.SparseTopKCategoricalAccuracy(top_k, name=\"top-5-accuracy\"),\n","    ],\n",")\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-12-11T18:59:51.679974Z","iopub.status.busy":"2022-12-11T18:59:51.679612Z","iopub.status.idle":"2022-12-12T00:44:53.221658Z","shell.execute_reply":"2022-12-12T00:44:53.220629Z","shell.execute_reply.started":"2022-12-11T18:59:51.679945Z"},"trusted":true},"outputs":[],"source":["history = model.fit(\n","    tr_dataset,\n","    batch_size=batch_size,\n","    epochs=num_epochs,\n","    validation_data=val_dataset,\n",")\n","history.save_weights('./model_weights')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
